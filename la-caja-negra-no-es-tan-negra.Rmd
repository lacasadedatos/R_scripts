---
title: "¬°La caja negra no es tan negra!"
description: |
  Algunos algoritmos de Aprendizaje Autom√°tico son tan complejos que entender c√≥mo llegan a la predicci√≥n de una categor√≠a o valor de regresi√≥n se convierte en una caja negra. La librer√≠a DALEX de R nos permite asomarnos a esa caja negra, entender c√≥mo funcionan estos modelos complejos y poder emitir recomendaciones precisas y cuantificables para mejorar los valores de nuestra variable de respuesta. Es fascinante...
author:
  - name: Erick Garc√≠a-Garc√≠a, Ph.D.
date:  "Feb. 20, 2026"
categories: [R, Regression, Random Forest, DALEX, GMLNet, XGBoost, predictive models, variable importance]
output:
  distill::distill_article:
    toc: true
    toc_float: true
    toc_depth: 3
    collapsed: true
    smooth_scroll: true
    #theme: spacelab
    highlight: pygments
    df_print: paged
    #code_folding: show
    self_contained: false
    code_download: true
---

```{css, echo=FALSE}

.infobox {
  padding: 1em 1em 1em 1em;
  border: 2px solid #4f9ed4;
  border-radius: 10px;
  background: ##a3cde5 5px center/3em no-repeat;
  line-height: 1.2;
}

.fig_legend {
  font-size: 0.8rem;
  line-height: 1.2;
}

.long_table {
  font-size: 0.9rem;
  line-height: 1;
}

.h2_heading {
  font-size: 18px;
  line-height: 1;
}
```

# ¬øPor qu√© es tan negra la caja?

Algunos algoritmos de Aprendizaje Aautom√°tico son tan complejos, que los procesos internos para alcanzar una predicci√≥n son dif√≠ciles de interpretar. Aunque estos modelos pueden predecir con muy alta presici√≥n categor√≠as o valores de una regresi√≥n, no explican el razonamiento subyacente ni las variables que influyeron en cada decisi√≥n.

Fuera del mundo acad√©mico esta "falta de transparencia" genera desconfianza, especialmente en situaciones en las que se necesita entender c√≥mo se alcanz√≥ a una decisi√≥n.

```         
```

::: {.infobox data-latex=""}
> ü§î La falta de **interpretabilidad** de algunos modelos de Aprendizaje Autom√°tico genera problemas de confianza, especialmente en aplicaciones cr√≠ticas como medicina, justicia o finanzas, donde es fundamental entender **c√≥mo** se tom√≥ una determinada decisi√≥n; por ejemplo, clasificar a un paciente como "en riesgo", conceder un pr√©stamo, etc.
:::

```         
```

Otros algoritmos, como la [regresi√≥n log√≠stica](https://lacasadedatos.github.io/blog/blog_posts/2026-01-25-ms-vale-diablo-conocido/#qu%C3%A9-historias-nos-cuenta-el-diablo-viejo "Transparencia de la Regresi√≥n log√≠stica") y modelos de regresi√≥n lineal, son m√°s transparentes y nos permiten entender su proceso de toma de decisiones de forma inmediata. Aunque este tipo de modelos predictivos se consideran menos poderosos, suelen ser mucho m√°s veloces y, dependiendo de los datos y el contexto, pueden alcanzar una capacidad predictiva muy similar a la de modelos m√°s complejos (ver nuestro *post* "[*¬øM√°s sabe el diablo por viejo?...*](https://lacasadedatos.github.io/blog/blog_posts/2026-01-25-ms-vale-diablo-conocido/)").

## Mirando dentro de la caja {style="font-size: 18px"}

La librer√≠a [DALEX](https://cran.r-project.org/web/packages/DALEX/DALEX.pdf "Dalex documentation") (Descriptive mAchine Learning EXplanations) de [R](https://www.r-project.org/ "R"), nos permite asomarnos dentro de la caja y entender el proceso de toma de decisiones de algoritmos complejos.

DALEX es una librer√≠a de R dise√±ada para **explicar e interpretar** modelos de Aprendizaje autom√°tico. Proporciona herramientas de interpretaci√≥n que funcionan con cualquier algoritmo (bosques de √°rboles aleatorios, redes neuronales, XGBoost, etc.).

Como veremos en este *post*, estas herramientas nos permiten crear visualizaciones intuitivas para entender tanto predicciones individuales como el comportamiento global del modelo.

## Los datos dentro de la caja {style="font-size: 18px"}

En esta ocasi√≥n trabajaremos con el conjunto de datos "[Kidney Function Health](https://www.kaggle.com/datasets/miadul/kidney-function-health-dataset?select=kidney_dataset.csv "ver datos")" de Kaggle. Este conjunto de datos es una recopilaci√≥n sint√©tica, pero realista desde el punto de vista m√©dico, de indicadores de salud relacionados con el funcionamiento del ri√±√≥n que son utilizados por los nefr√≥logos para evaluar la funci√≥n renal, detectar la enfermedad renal cr√≥nica temprana y evaluar los factores de riesgo. Este conjunto de datos incluye 5.000 registros de pacientes con 11 caracter√≠sticas cl√≠nicamente relevantes:

| Variable | Descripci√≥n |
|------------------------------------|------------------------------------|
| Creatinina | Nivel de creatinina en sangre (mg/dL). Valores altos indican una filtraci√≥n renal reducida. |
| BUN | Nitr√≥geno ur√©ico en sangre (mg/dL). Niveles elevados indican funci√≥n renal comprometida. |
| GFR (Objetivo para regresi√≥n) | Tasa de filtraci√≥n glomerular (5‚Äì120). Valores m√°s altos indican mejor funci√≥n renal. |
| Salida de orina | Volumen diario de orina (mL/d√≠a). Bajo volumen puede reflejar da√±o renal. |
| Diabetes | Presencia de diabetes; un factor de riesgo importante para la enfermedad renal cr√≥nica. |
| Hipertensi√≥n | Estado de presi√≥n arterial alta, fuertemente asociado con la progresi√≥n de la enfermedad renal cr√≥nica. |
| Edad | Edad del paciente (18‚Äì90). La funci√≥n renal disminuye con la edad. |
| Prote√≠na en la orina | Nivel de proteinuria (mg/dL). Alta prote√≠na = da√±o renal. |
| Ingesta de agua | Consumo diario de agua (litros/d√≠a). |
| Medicaci√≥n | Tipo de medicaci√≥n: Ninguna, Inhibidor de ECA, ARB, Diur√©tico. |

El objetivo de nuestros modelos predictivos de regresi√≥n ser√° predecir la tasa de filtraci√≥n gromerular (GFR, Glomerular Filtration Rate) a partir de las variables mostradas arriba.

Crearemos algunos modelos predictivos de Aprendizaje Autom√°tico y luego nos asomaremos a si interior con las herramientas de la librer√≠a DALEX.

```{r include=FALSE}
#| label: load libraries
#| message: false
#| warning: false
library(tidymodels)
library(tidyverse)
library(explore)
library(dlookr)
library(gridExtra)
library(baguette)
library(flextable)
library(DALEX)
library(ggplot2)

`%notin%` <- Negate(`%in%`)
```

```{r include=FALSE}
#| label: load data
#| message: false
#| warning: false
#| cache: TRUE
DF <- read_csv("https://raw.githubusercontent.com/lacasadedatos/datasets/refs/heads/main/kidney_GFR.csv") %>% 
  mutate_if(is.character, as.factor)


# Reorganize the DF
DF <- DF %>% select(GFR, CKD_Status, everything())
glimpse(DF, width = 50)
```

## Breve exploraci√≥n de nuestros datos {style="font-size: 18px"}

La Figura 1 muestra la relaci√≥n de todas nuestras variables con la tasa de filtraci√≥n glomerular (GFR), que ser√° la variable objetivo de nuestros modelos de regresi√≥n.

```{r echo=FALSE}
#| message: false
#| warning: false
#| cache: TRUE
start <- 2 # 1st col
end <- 5 # last col
target <-1 # target col

DF[c(start:end, target)] %>%
  explore_all(target = GFR)

```

```{r echo=FALSE}

#| message: false
#| warning: false
#| cache: TRUE
start <- 6 # 1st col
end <- 9 # last col
target <-1 # target col

DF[c(start:end, target)] %>% 
  explore_all(target = GFR)# name of the target
```

```{r echo=FALSE}
#| message: false
#| warning: false
#| cache: TRUE
start <- 10 # 1st col
end <- 11 # last col
target <-1 # target col

DF[c(start:end, target)] %>% 
  explore_all(target = GFR)# name of the target
```

::: {.fig_legend data-latex=""}
**Figura 1**. Relaci√≥n de nuestras variables predictoras con la tasa de filtraci√≥n glomerular (GFR, Glomerular Filtration Rate). CKD_Status, enfermedad renal cr√≥nica; Creatinine, creatinina; BUN, Nitr√≥geno ur√©ico en sangre; urine_Output, salida de orina; Diabetes, diabetes; Hypertension, hipertensi√≥n; Age, edad; Protein_in_Urine, prote√≠na en orina; Water_Intake, ingesta de agua; Medication, tipo de medicaci√≥n.
:::

En la Figura 1 podemos ver que existe una fuerte relaci√≥n entre la tasa de filtraci√≥n glomerular y la presencia de enfermedad renal cr√≥nica (CKD), el nivel de creatinina, el nitr√≥geno ur√©ico en sangre, la salida de orina, y posiblemente la presencia de prote√≠na en la orina.

# Creaci√≥n de distintos modelos predictivos

El proceso de creaci√≥n de algoritmos de Aprendizaje Autom√°tico involucra m√∫ltiples pasos: c) limpieza y escalado de los datos, b) separaci√≥n de conjuntos de datos de entrenamiento y testeo, c) programaci√≥n de los modelos, d) an√°lisis de m√©tricas de desempe√±o con los datos de entrenamiento para descartar [sobreajuste](https://www.baeldung.com/cs/random-forest-overfitting-fix "overfitting in ML"), e) comparaci√≥n de distintas m√©tricas de despempe√±o entre modelos para elegir el mejor o los mejores en la fase de entrenamiento, y pruebas de desempe√±o con los datos de testeo.

Los modelos de aprendizaje autom√°tico creados fueron: [regresi√≥n lineal](https://www.geeksforgeeks.org/machine-learning/ml-linear-regression/), [GLMNet](https://trevorhastie.r-universe.dev/articles/glmnet/glmnet.html "GLMNet"), bosque de √°rboles aleatorios ([*random forest*](https://towardsmachinelearning.org/random-forest/ "random forest")), [XGBoost](https://www.geeksforgeeks.org/machine-learning/xgboost/ "XGBoost") y SVM ([Supported Vector Machines](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/ "SVM")). Todos estos modelos se crearon utilizando la metalibrer√≠a [Tidymodels](https://www.tidymodels.org/) de R.

A continuaci√≥n te ofrecemos una muy breve explicaci√≥n de c√≥mo funciona cada algoritmo.

-   Regresi√≥n Lineal: es una t√©cnica estad√≠stica que modela la relaci√≥n entre una variable dependiente y una o m√°s variables independientes. Busca encontrar la l√≠nea recta que mejor se ajuste a los datos, permitiendo predecir valores de la variable dependiente en funci√≥n de las independientes. Es √∫til para identificar tendencias y patrones.
-   GLMNet: es un modelo de aprendizaje autom√°tico que utiliza el Modelo Lineal Generalizado (GLM) para predecir resultados. GLMNet combina la [regularizaci√≥n](https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf) L1 (Lasso) y L2 (Ridge) en un √∫nico modelo, a√±adiendo un par√°metro lambda para controlar la fuerza de la regularizaci√≥n L1 y un par√°metro alpha para balancear entre L1 y L2.
-   Bosque de √°rboles aleatorios: utiliza m√∫ltiples [√°rboles de decisi√≥n](https://www.geeksforgeeks.org/machine-learning/decision-tree/) para hacer predicciones. Cada √°rbol en el bosque hace una predicci√≥n basada en los datos de entrada y la predicci√≥n final se hace combinando las predicciones de todos los √°rboles.
-   XGBoost: construye una serie de modelos secuenciales haciendo predicciones con √°rboles de decisi√≥n de bajo poder predictivo (√°rboles d√©biles); cada nuevo modelo se entrena enfoc√°ndose en los errores del anterior. Este proceso iterativo corrige gradualmente las predicciones incorrectas, creando un modelo final m√°s fuerte y preciso que supera significativamente el rendimiento individual de cada √°rbol d√©bil.

La fase de entrenamiento de cada modelo se realiz√≥ con validaci√≥n cruzada con 5 particiones de datos y exploraci√≥n automatizada de hiperpar√°metros (variables matem√°ticas que modulan el funcionamiento de un algoritmo, definidas por el usuario).

Para centrar nuestra atenci√≥n en las herramientas de interpretaci√≥n de modelos de Aprendizaje Autom√°tico que nos ofrece la librer√≠a DALEX, nos saltaremos losm resultados de la fase de entrenamiento y partiremos de los resultados de desempe√±o de nuestros modelos en la fase final de testeo. En todo caso, puedes ver el script de programaci√≥n completo en el repositorio de [La Casa de Datos](https://github.com/lacasadedatos/R_scripts "Repositorio La Casa de Datos") en GitHub.

Como estamos trabajando con modelos de regresi√≥n, una forma de visualizar el desempe√±o de nuestros modelos es graficando el valor estimado de la variable de respuesta (la prediccion sobre la tasa de filtraci√≥n glomerular, GFR) frente a los valores reales (Figura 2).

```{r}
#| include: FALSE

## Data splitting (rsample) ----
set.seed(1976)
data_split <- initial_split(data = DF, prop = 0.75, strata = GFR)
training_data <- training(data_split)
testing_data <- testing(data_split)


## cross validation on the training data ----
set.seed(1976)
myfolds <- vfold_cv(training_data,
  v = 5, # number of folds
  strata = GFR
)

## Recipe ----
R_CorrOk <- recipe(GFR ~ ., data = training_data) %>%
		step_normalize(all_numeric(), -all_outcomes()) %>% # scaling
  	step_zv(all_numeric(),  -all_outcomes()) %>% # remove zero variance vars
    step_dummy(all_nominal_predictors()) # create dummy vars for nominal vars


## Apply recipe: get pre-processed data for inspection ----

training_CorrOk <- R_CorrOk %>%
		prep(training = training_data) %>%
		bake(new_data = NULL)


testing_CorrOk <- R_CorrOk %>%
		prep(training = training_data) %>%
		bake(new_data = testing_data) %>% 
    select(GFR, everything())

```

```{r}
#| label: linear_regression
#| include: FALSE
#| cache: TRUE
#| message: false
#| warning: false
# model setup ----
M_linearReg <- linear_reg() %>% # model type:
# https://www.tidymodels.org/find/parsnip/
  set_engine('glm') %>% # specific R package
  set_mode('regression') # Set mode

wkfl_linearReg <- workflow() %>%
		add_model(M_linearReg) %>%
		add_recipe(R_CorrOk)


rs_linearReg <- wkfl_linearReg %>%
		fit_resamples(resamples = myfolds)

# inspect output for high variability ----
rs_linearReg  %>% collect_metrics(summarize = FALSE) %>%
  group_by(.metric) %>% # customize
  summarize(
    mean = mean(.estimate),
    min = min(.estimate), # customize
    median = median(.estimate), # customize
    max = max(.estimate) # customize
  )

# metrics on the testing data ----

lf_linearReg_CorrOk <- workflow() %>%
		add_model(M_linearReg) %>%
		add_recipe(R_CorrOk) %>% 
    last_fit(split = data_split)


finalM_lR <-lf_linearReg_CorrOk %>%
	collect_metrics()



```

```{r}
#| echo: false
#| message: false
#| warning: false
# plot ----
pred_linearReg_CorrOk <- lf_linearReg_CorrOk %>% 
		collect_predictions()

lr_plot <- ggplot(pred_linearReg_CorrOk, aes(
  pred_linearReg_CorrOk$GFR,
  pred_linearReg_CorrOk$.pred
)) +
  geom_point(alpha = 0.5) + # create scatter plot
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "True GFR value", y = "Predictions") + # st se = TRUE to show se area
  theme(
  axis.title.x = element_text(size = 18),  # X-axis title
  axis.title.y = element_text(size = 18)   # Y-axis title
) +
  ggtitle("A) Linear Regression")
```

```{r}
#| label: glmnet
#| include: FALSE
#| cache: TRUE
#| message: false
#| warning: false

# glmnet::glmnet() uses regularized least squares to fit models with numeric # outcomes.
# 
# For this engine, there is a single mode: regression
# 
# This model has 2 tuning parameters:
# 
# penalty: Amount of Regularization (type: double, default: see below)
# 
# mixture: Proportion of Lasso Penalty (type: double, default: 1.0)
# 
# A value of mixture = 1 corresponds to a pure lasso model, while mixture = 0 # indicates ridge regression.
# 
# The penalty parameter has no default and requires a single numeric value. For # more details about this, and the glmnet model in general, see glmnet-details.
# 
# For this model we will use the full model with normalization (Recipe 1)

## Model definition ----
M_GLMNet <- linear_reg() %>% # model type:
# https://www.tidymodels.org/find/parsnip/
  set_engine('glmnet') %>% # specific R package
  set_mode('regression') # Set mode

## create a worlflow ----
wkfl_GLMNet <- workflow() %>%
		add_model(M_GLMNet) %>%
		add_recipe(R_CorrOk) # full model with normalization


## Create a tuning model ----
# Models' names and hyperparameters are in:
# https://www.tidymodels.org/find/parsnip/
tune_GLMNet <- linear_reg(
  penalty = tune(), # ML algorithm
  mixture = tune()
) %>%
  set_engine("glmnet") %>%
  set_mode("regression")
## Add tuning to the original workflow
tuned_wkfl_GLMNet <- wkfl_GLMNet %>%
  update_model(tune_GLMNet)

## create a tuning grid with random values
set.seed(1976)
grid_GLMNet <- tune_GLMNet %>%
  extract_parameter_set_dials() %>%
  grid_random(., size = 5)

## Apply the tune grid to the workflow with cross-validation
tuning_GLMNet <- tuned_wkfl_GLMNet %>%
  tune_grid(
    resamples = myfolds,
    grid = grid_GLMNet
  )
# Check results ----
tuneResults_GLMNet <- tuning_GLMNet %>%
  collect_metrics()

tuning_GLMNet %>%
  collect_metrics(summarize = FALSE) %>%
  group_by(.metric) %>% # customize
  summarize(
    mean = mean(.estimate),
    min = min(.estimate), # customize
    median = median(.estimate), # customize
    max = max(.estimate) # customize
  )


## Best performing models ----
# top n hyperparameter combinations according to "metric"
(tuning_GLMNet %>% 
	show_best(metric = "rmse", n = 5)) 


## Select the best performing model ----
# Outputs a tibble with the best hyperparameter combination
(best_GLMNet <- tuning_GLMNet %>% 
					select_best(metric = "rmse"))


## Finalize the workflow with last_fit() ----
finalWkfl_GLMNet <- tuned_wkfl_GLMNet %>%
					finalize_workflow(best_GLMNet)
					
finalTuned_GLMNet <- finalWkfl_GLMNet %>%
					last_fit(data_split)
					
## View final results performed on the testing_data ----
(finalM_GLMNet <- finalTuned_GLMNet %>%
		collect_metrics())

## Get predictions on the testing data ----
preds_GLMNet <- finalTuned_GLMNet %>%
  collect_predictions()



```

```{r}
#| echo: false
#| message: false
#| warning: false
## plot predictions vs true outcomes ----

glmnet_plot <-ggplot(preds_GLMNet, aes(preds_GLMNet$GFR,
                                         preds_GLMNet$.pred)) +
  geom_point(alpha = 0.5) + # create scatter plot
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "True GFR value", y = "Predictions") +# st se = TRUE to show se area
  theme(
  axis.title.x = element_text(size = 18),  # X-axis title
  axis.title.y = element_text(size = 18)   # Y-axis title
) +
  ggtitle("B) GLMNet")
```

```{r}
#| label: random_forest
#| include: FALSE
#| cache: TRUE
#| message: false
#| warning: false
# ## Model definition ----
M_rForest <- rand_forest() %>% # model type:
# https://www.tidymodels.org/find/parsnip/
# show_engines("boost_tree")for available engines.
  set_engine('randomForest') %>% # specific R package
  set_mode('regression') # Set mode

## create a workflow ----
wkfl_rForest <- workflow() %>%
		add_model(M_rForest) %>%
		add_recipe(R_CorrOk) # full model with normalization


## Create a tuning model
# Models' names and hyperparameters are in:
# https://www.tidymodels.org/find/parsnip/
# call show_model_info("rand_forest") to show hyperparameters
tune_rForest <- rand_forest(
  mtry = tune(), # ML algorithm
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("randomForest") %>%
  set_mode("regression")

## Add tuning to the original workflow
tuned_wkfl_rForest <- wkfl_rForest %>%
		update_model(tune_rForest)

## create a tuning grid with random values
set.seed(1976)
grid_rForest <- tune_rForest %>%
		extract_parameter_set_dials() %>%
    # mtry needs extra info from the pre-processed data, so:
    finalize(training_CorrOk) %>%
		grid_random(., size = 5)

## Apply the tune grid to the workflow with cross-validation
tuning_rForest <- tuned_wkfl_rForest %>%
		tune_grid(resamples = myfolds,
				  grid = grid_rForest)
# Check results
tuneResults_rForest <- tuning_rForest %>%
	collect_metrics()

(tuning_rForest %>%
  collect_metrics(summarize = FALSE) %>%
  group_by(.metric) %>% # customize
  summarize(
    mean = mean(.estimate),
    min = min(.estimate), # customize
    median = median(.estimate), # customize
    max = max(.estimate) # customize
  ))


## Best performing models ----
# top n hyperparameter combinations according to "metric"
(tuning_rForest %>% 
	show_best(metric = "rmse", n = 5)) 


## Select the best performing model ----
# Outputs a tibble with the best hyperparameter combination
(best_rForest <- tuning_rForest %>% 
					select_best(metric = "rmse"))


## Finalize the workflow with last_fit() ----
finalWkfl_rForest <- tuned_wkfl_rForest %>%
					finalize_workflow(best_rForest)
					
finalTuned_rForest <- finalWkfl_rForest %>%
					last_fit(data_split)
					
## View final results performed on the testing_data ----
(finalM_rForest <- finalTuned_rForest %>%
		collect_metrics())

preds_rForest <- finalTuned_rForest %>%
  collect_predictions()



```

```{r}
#| echo: false
#| message: false
#| warning: false
## plot predictions vs true outcomes ----

rForest_plot <-ggplot(preds_rForest, aes(preds_rForest$GFR,
                                         preds_rForest$.pred)) +
  geom_point(alpha = 0.5) + # create scatter plot
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "True GFR value", y = "Predictions") +# st se = TRUE to show se area
  theme(
  axis.title.x = element_text(size = 18),  # X-axis title
  axis.title.y = element_text(size = 18)   # Y-axis title
) +
  ggtitle("C) Random Forest")
```

```{r}
#| label: xgboost
#| include: FALSE
#| cache: TRUE
#| message: false
#| warning: false
## Model definition ----
M_xgboost <- boost_tree() %>% # model type:
# https://www.tidymodels.org/find/parsnip/
# show_engines("boost_tree")for available engines.
  set_engine('xgboost') %>% # specific R package
  set_mode('regression') # Set mode

## create a workflow ----
wkfl_xgboost <- workflow() %>%
		add_model(M_xgboost) %>%
		add_recipe(R_CorrOk) # full model with normalization


## Create a tuning model ----
# call show_model_info("boost_tree") to show hyperparameters
tune_xgboost <- boost_tree(
  tree_depth = tune(),
  trees = tune(),
  learn_rate = tune(),
  mtry = tune(),
  min_n = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  stop_iter = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

## Add tuning to the original workflow
tuned_wkfl_xgboost <- wkfl_xgboost %>%
		update_model(tune_xgboost)

## create a tuning grid with random values
set.seed(1976)
grid_xgbost <- tune_xgboost %>%
		extract_parameter_set_dials() %>%
    # mtry needs extra info from the pre-processed data, so:
    finalize(training_CorrOk) %>%
		grid_random(., size = 5)

## Apply the tune grid to the workflow with cross-validation
tuning_xgboost <- tuned_wkfl_xgboost %>%
		tune_grid(resamples = myfolds,
				  grid = grid_xgbost)
# Check results
tuneResults_xgboost <- tuning_xgboost %>%
	collect_metrics()

(tuning_xgboost %>%
  collect_metrics(summarize = FALSE) %>%
  group_by(.metric) %>% # customize
  summarize(
    mean = mean(.estimate),
    min = min(.estimate), # customize
    median = median(.estimate), # customize
    max = max(.estimate) # customize
  ))


## Best performing models ----
# top n hyperparameter combinations according to "metric"
(tuning_xgboost %>% 
	show_best(metric = "rmse", n = 5)) 


## Select the best performing model ----
# Outputs a tibble with the best hyperparameter combination
(best_xgboost <- tuning_xgboost %>% 
					select_best(metric = "rmse"))


## Finalize the workflow with last_fit() ----
finalWkfl_xgboost <- tuned_wkfl_xgboost %>%
					finalize_workflow(best_xgboost)
					
finalTuned_xgboost <- finalWkfl_xgboost %>%
					last_fit(data_split)
					
## View final results performed on the testing_data ----
(finalM_xgboost <- finalTuned_xgboost %>%
		collect_metrics())

preds_xgboost <- finalTuned_xgboost %>%
  collect_predictions()



```

```{r}
#| echo: false
#| message: false
#| warning: false
## plot predictions vs true outcomes ----

xgboost_plot <- ggplot(preds_xgboost, aes(preds_xgboost$GFR,
                                          preds_xgboost$.pred)) +
  geom_point(alpha = 0.5) + # create scatter plot
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "True GFR value", y = "Predictions") +
  theme(
  axis.title.x = element_text(size = 18),  # X-axis title
  axis.title.y = element_text(size = 18)   # Y-axis title
) +
  ggtitle("D) XGBoost")# st se = TRUE to show se area
```

```{r}
#| echo: false
#| message: false
#| warning: false
lr_plot
glmnet_plot
rForest_plot
xgboost_plot
```

::: {.fig_legend data-latex=""}
**Figura 2**. Correlaci√≥n entre las predicciones de cada modelo (Predictions) y los valores reales de la tasa de filtraci√≥n glomerular real (True GFR vakue). La linea azul corresponde a la l√≠nea de la regresi√≥n y cada punto es un caso.
:::

En la Figura 2 vemos que no todos los modelos tuvieron el mismo desempe√±o; en las gr√°ficas, las predicciones de los modelos de regresi√≥n lineal y GMLNet son los que m√°s se alejan de la l√≠nea de regresi√≥n. Los datos que mejor se ajustan a la l√≠nea de regresi√≥n son los del modelo XGBoost, seguidos de los del bosque de √°rboles aleatorios.

Podemos cuantificar c√≥mo de acertadas son las predicciones de cada modelo usando la Ra√≠z del Error Cuadr√°tico Medio de las predicciones ([RMSE](https://www.datacamp.com/tutorial/rmse "RMSE explicado")). Podemos adem√°s estimar c√≥mo de bueno es nuestro modelo usando la [R¬≤](https://www.geeksforgeeks.org/machine-learning/ml-r-squared-in-regression-analysis/ "R¬≤ explained") que nos indica la proporci√≥n de la variabilidad de nuestros datos que el modelo puede explicar.

La Tabla 1 muestra los resultados de la Ra√≠z del Error Cuadr√°tico Medio; como vemos, los valores de error m√°s bajos corresponden al modelo XGBoost, seguidos del bosque de √°rboles aleatorios.

```{r}
#| include: FALSE
#| echo: FALSE
models <-  c(
  rep("Regresi√≥n log√≠stica", 2),
  rep("GLMNet", 2),
  rep("B. de √°rboles aleatorios", 2),
   rep("XGBoost", 2)
) %>%
  as_tibble() %>%
  setNames("Modelo")

finalM_lR_metrics <- finalM_lR %>% select(c(.metric, .estimate)) %>% setNames(c("M√©trica", "Valor"))
finalM_GLMNet_metrics <- finalM_GLMNet %>% select(c(.metric, .estimate)) %>% setNames(c("M√©trica", "Valor"))
finalM_rForest_metrics <- finalM_rForest %>% select(c(.metric, .estimate)) %>% setNames(c("M√©trica", "Valor"))
finalM_xgboost_metrics <- finalM_xgboost %>% select(c(.metric, .estimate)) %>% setNames(c("M√©trica", "Valor"))

models_summary <- bind_rows(
finalM_lR_metrics,
finalM_GLMNet_metrics,
finalM_rForest_metrics,
finalM_xgboost_metrics
) %>% bind_cols(models, .) %>% arrange(M√©trica, Valor)

```

```{r}
#| echo: FALSE
models_summary[1:4,] %>% flextable() %>%
  add_header_lines("Tabla 1. Ra√≠z del error cuadr√°tico medio (rmse).") %>%
  theme_zebra() %>%
  autofit()
```

La Tabla 2 muestra los resultados de R¬≤. Aunque todos los modelos explican m√°s del 99% de la variabilidad de los datos, los valores m√°s altos corresponden al modelo XGBoost, seguidos del bosque de √°rboles aleatorios.

```{r}
#| echo: FALSE
models_summary[5:8,] %>% arrange(desc(Valor)) %>% 
  flextable() %>%
  add_header_lines("Tabla 2. R¬≤ (srq).") %>%
  theme_zebra() %>%
  autofit()
```

```         
```

::: {.infobox data-latex=""}
> üöÄ Los mejores modelos para nuestro conjunto de datos son XGBoost y el bosque de √°rboles aleatorios, ya que tienen los valores m√°s bajos de error asociado a la regresi√≥n.
:::

```         
```

# Mirando el interior de la caja

Como ya vimos en un *post* anterior ([*¬øM√°s sabe el diablo por viejo...?*](https://lacasadedatos.github.io/blog/blog_posts/2026-01-25-ms-vale-diablo-conocido/)), a diferencia de modelos sencillos como la regresi√≥n lineal o la regresi√≥n log√≠stica, los [res√∫menes de modelos complejos](https://lacasadedatos.github.io/blog/blog_posts/2026-01-25-ms-vale-diablo-conocido/#qu%C3%A9-historias-nos-cuenta-el-diablo-viejo) suelen ser bastante cr√≠pticos y no parecen nada √∫tiles para entender c√≥mo un modelo de Aprendizaje Autom√°tico alcanza una predicci√≥n.

A continuaci√≥n usaremos la librer√≠a DALEX para explorar y entender la forma en la que XGBoost y el bosque de √°rboles aleatorios, nuestros mejores modelos para este conjunto de datos, trabajan para alcanzar una predicci√≥n.

DALEX utiliza una interfaz unificada mediante para explicar modelosdeAprendizaje Autom√°tico complejos, haciendo transparentes los modelos de caja negra. El objeto de programaci√≥n base de esta librer√≠a se denomina "explainer".

El c√≥digo que se muestra a continuaci√≥n corresponde a la creaci√≥n de los "explainers" para XGBoost y el bosque de √°rboles aleatorios.

```{r}
#| message: false
#| warning: false
#| include: FALSE
#| cache: TRUE
#| echo: FALSE
xgboost_model <-  finalTuned_xgboost %>% # call the tidymodel's model object
  extract_fit_parsnip() # get model details

xgboost_explainer <- explain(xgboost_model,
              data = as.data.frame(testing_CorrOk[, 2:13]), # preprocessed data
              y = as.numeric(testing_CorrOk$GFR), # outcome variable
              label = "")

randomForest_model <-  finalTuned_rForest %>% # call the tidymodel's model object
  extract_fit_parsnip() # get model details

randomForest_explainer <- explain(randomForest_model,
              data = as.data.frame(testing_CorrOk[, 2:13]), # preprocessed data
              y = as.numeric(testing_CorrOk$GFR), # outcome variable
              label = "")
```

Una vez creados estos "explainers", contamos con distintas herramientas para explicar el proceso de decisi√≥n de cada modelo.

## üåê Funcionamiento global de los modelos: importancia relativa de las variables predictoras {style="font-size: 18px"}

Los gr√°ficos de importancia de variables ([Variable Importance Plots](https://r-packages.io/packages/DALEX/model_parts)) muestran qu√© variables tienen mayor impacto en las predicciones de un modelo predictivo de Aprendizaje Autom√°tico. DALEX calcula la importancia de cada variable dentro del modelo mediante permutaciones; mezcla aleatoriamente los valores de cada variable y mide c√≥mo se degradan las m√©tricas de desempe√±o (por defecto, 1-AUC para clasificaci√≥n y RMSE para regresi√≥n). Cuando se modifica una variable, una mayor degradaci√≥n de la m√©trica de desempe√±o es indicativa de mayo importancia de esa variable. Los gr√°ficos VIP permiten identificar f√°cilmente las variables que son clave para un modelo predictivo.

La Figura 3 muestra los gr√°ficos VIP para el modelo XGBoost y el bosque de √°rboles aleatorios, con las variables m√°s importantes para cada modelo.

```{r}
#| echo: false
#| cache: TRUE
#| include: FALSE

xgboost_VIP <- xgboost_explainer %>%
  model_parts() %>%
  plot() # show_boxplots = FALSE optional argument

randomForest_VIP <- randomForest_explainer %>%
  model_parts() %>%
  plot() 
```

```{r}
#| echo: FALSE
#| cache: FALSE
xgboost_VIP +
  theme(
  axis.title.x = element_text(size = 16),
  axis.title.y = element_text(size = 16)
)+
  theme(
  plot.title = element_text(color = "black", size = 18) 
) +
  theme_minimal() +
  theme(plot.subtitle = element_blank())+
  theme(legend.position = "none") +
  ggtitle("A) XGBoost")

randomForest_VIP +
  theme(
  axis.title.x = element_text(size = 16),
  axis.title.y = element_text(size = 16)
)+
  theme(
  plot.title = element_text(color = "black", size = 18) 
) +
  theme_minimal() +
  theme(plot.subtitle = element_blank())+
  theme(legend.position = "none") +
  ggtitle("B) Random Forest")
```

::: {.fig_legend data-latex=""}
**Figura 3**. Gr√°ficos tipo VIP para los modelos predictivos XGBoost y bosque de √°rboles aleatorios. La importancia relativa de cada variable fue estimada a partir la disminuci√≥n del error cuadr√°tico medio de las predicciones, tras las permutaciones de las variables de entrada (RMSE loss after permutations). Creatine, creatinina; BUN, Nitr√≥geno ur√©ico en sangre; Protein_in_Urine, prote√≠na en orina; CKD_Status_Yes, presencia de enfermedad renal cr√≥nica; Age, edad; Diabetes_Yes, presencia de diabetes; Hypertension_Yes, presencia de hipertensi√≥n; Urine_Output, salida de orina; Water_Intake, ingesta de agua; Medication, tipo de medicaci√≥n (ARB; Diuretic, diur√©ticos; None, ninguna).
:::

Como podemos ver en la Figura 3, cada modelo asigna una importancia relativa distinta a cada variable; sin embargo, las variables m√°s importantes son las mismas en los dos modelos. Estos resultados coinciden con la exploraci√≥n visual de nuestros datos (Figura 1), lo que resulta muy tranquilizador.

```         
```

::: {.infobox data-latex=""}
> üîç Los gr√°ficos tipo VIP nos ayudan a entender de un vistazo las variables que contribuyen en mayor medida a la capacidad de un modelo de Aprendizaje Autom√°tico para arrojar una predicci√≥n.
:::

```         
```

## üß© Ahora pieza por pieza: an√°lisis de casos individuales {style="font-size: 18px"}

Ya tenemos un modelo predictivo, ¬°qu√© bonito! Contamos con gr√°ficos tipo VIP para entender las variables que son importantes para nuestro modelo, ¬°qu√© emoci√≥n! Pero... ¬øPodemos hacer algo incluso mejor? ¬øTal vez explicar a un paciente EXACTAMENTE cu√°les de sus caracter√≠sticas individuales podr√≠an predecir niveles una tasa de filtraci√≥n glomerular preocupante?

De nuestros 5000 casos, elegimos dos sujetos al azar, con tasas de filtraci√≥n glomerular (GFR) baja (Low) o normal (Normal), para el an√°lisis explicativo de nuestros modelos a nivel de caso √∫nico.

::: {.long_table data-latex=""}
| GFR          | Variable         | Valor    |
|--------------|------------------|----------|
| Low (5)      | Age              | 46.96    |
| Normal (103) | Age              | 18       |
| Low (5)      | BUN              | 107.61   |
| Normal (103) | BUN              | 15.51    |
| Low (5)      | CKD_Status       | Yes      |
| Normal (103) | CKD_Status       | No       |
| Low (5)      | Creatinine       | 5.83     |
| Normal (103) | Creatinine       | 0.64     |
| Low (5)      | Diabetes         | Yes      |
| Normal (103) | Diabetes         | No       |
| Low (5)      | Hypertension     | No       |
| Normal (103) | Hypertension     | No       |
| Low (5)      | Medication       | None     |
| Normal (103) | Medication       | Diuretic |
| Low (5)      | Protein_in_Urine | 1058.17  |
| Normal (103) | Protein_in_Urine | 52.62    |
| Low (5)      | Urine_Output     | 453.55   |
| Normal (103) | Urine_Output     | 1581.49  |
| Low (5)      | Water_Intake     | 2.24     |
| Normal (103) | Water_Intake     | 2.85     |
:::

:::: {.fig_legend data-latex=""}
::: {.fig_legend data-latex=""}
GFR, tasa de filtraci√≥n glomerular; Age, edad; BUN, Nitr√≥geno ur√©ico en sangre; CKD_Status, enfermedad renal cr√≥nica; Creatinine, creatinina; Diabetes, diabetes; Hypertension, hipertensi√≥n; Medication, tipo de medicaci√≥n; Protein_in_Urine, prote√≠na en orina; Urine_Output, salida de orina; Water_Intake, ingesta de agua
:::
::::

```{r}
#| include: FALSE
# get data subsets with the lowest and highest GFR values
DF_lowGFR <- DF %>% arrange(GFR) %>% head(20) 

DF_highGFR <- DF %>% arrange(desc(GFR)) %>% head(20)

# select a random row from each subset
set.seed(87) 
instance_low <- DF_lowGFR[sample(nrow(DF_lowGFR), 1), ] # random row 
instance_high <- DF_highGFR[sample(nrow(DF_highGFR), 1), ] # random row

# combine two cases
subjects <- bind_rows(instance_low, instance_high)

# bake these new data for analysis

explainer_subjects <- R_CorrOk %>%
		prep(training = training_data) %>%
		bake(new_data = subjects) %>% 
    select(GFR, everything())

instance1 <- explainer_subjects[1,]
instance2 <- explainer_subjects[2,]

# the output is a DF with variables and their contributions
xgboost_breakdown <- predict_parts(explainer = xgboost_explainer, 
                    new_observation = instance1, type = "break_down")

xgboost_breakdown2 <- predict_parts(explainer = xgboost_explainer, 
                    new_observation = instance2, type = "break_down")
```

Las Figuras 4 y 5 muestras gr√°ficos de descomposici√≥n del valor estimado de la regresi√≥n para los dos sujetos, y para cada uno de los mejores modelos (XGBoost y bosque de √°rboles aleatorios). La funci√≥n `predict_parts()` de DALEX nos devuelve los datos necesarios para crear gr√°ficos barras con las contribuciones de cada variable al valor estimado de tasa de filtraci√≥n glomerular (GFR) para cada modelo.

El punto de partida en el gr√°fico es el t√©rmino independiente (intercept), que representa el valor predicho de la variable de respuesta cuando todas las variables predictoras son cero. Aunque el t√©rmino independiente no tiene mucho sentido en la gran mayor√≠a de los modelos de regresi√≥n, podemos visualizarlo como valor de la variable de respuesta sobre el que van a actuar las variables predictoras. En nuestro conjunto de datos el t√©rmino independiente de los dos modelos (73.42 con XGBoost y 73.39 con el bosque de √°rboles) se aproxima a la media de la tasa de filtraci√≥n glomerular de los 5000 casos que componen nuestra muestra (73.25).

```{r}
#| echo: FALSE
#| message: false
#| warning: false
xgboost_explainInstance <- xgboost_breakdown %>%
  as_tibble()
# Add label to variable name:
xgboost_explainInstance[nrow(xgboost_explainInstance), 3] <- "Prediction"
xgboost_explainInstance[1, 6] <- "0" # change "sing" of the intercept
xgboost_explainInstance <- xgboost_explainInstance %>%
  select(3, 2, 5, 6, 7) %>% 
  # reverse the order of position for better visualization after coord flip:
  mutate(position = factor(position, levels = rev(position))) %>% 
    mutate(sign = case_when( # make more intuitve factors
    sign == 0 ~"intercept",
    sign == -1 ~"negative",
    sign == 1 ~"positive",
    sign == "X" ~"Prediction"
  ))
  

labels <- rev(xgboost_explainInstance$variable_name)# labels for x axis
# labels above bars:
labels_aboveBars <- xgboost_explainInstance$contribution %>% 
  round(2) %>% 
  as.character() 
# labels_aboveBars[1] <- "-" # "delete intercept value
# labels_aboveBars[length(labels_aboveBars)] <- "-" # "delete" prediction value

ggplot(data = xgboost_explainInstance, mapping = aes(x = position, y = cumulative, fill = sign)) +
    geom_col()+
    coord_flip()+
    scale_x_discrete(labels = labels) +
    scale_fill_manual(values = c(
      "intercept" = "#e6cea0",
      "negative" = "#db4646",
      "positive" = "#24b3d0",
      "Prediction" = "#684b8e"
    ))+
    geom_text(aes(label = labels_aboveBars), 
          size = 4, 
          color = "black", 
          fontface = "bold")+
    xlab("Variable") +
    ylab("GFR")+
    theme_minimal() +
    theme(
  axis.title.x = element_text(size = 16),
  axis.title.y = element_text(size = 16)
)+
  theme(
  plot.title = element_text(color = "black", size = 18) 
) +
  theme(legend.position = "none") +
  ggtitle("A) Patient with low GFR")

```

```{r}
#| echo: FALSE
#| message: false
#| warning: false
xgboost_explainInstance2 <- xgboost_breakdown2 %>%
  as_tibble()
# Add label to variable name:
xgboost_explainInstance2[nrow(xgboost_explainInstance2), 3] <- "Prediction"
xgboost_explainInstance2[1, 6] <- "0" # change "sing" of the intercept
xgboost_explainInstance2 <- xgboost_explainInstance2 %>%
  select(3, 2, 5, 6, 7) %>% 
  # reverse the order of position for better visualization after coord flip:
  mutate(position = factor(position, levels = rev(position))) %>% 
    mutate(sign = case_when( # make more intuitve factors
    sign == 0 ~"intercept",
    sign == -1 ~"negative",
    sign == 1 ~"positive",
    sign == "X" ~"Prediction"
  ))
  

labels <- rev(xgboost_explainInstance2$variable_name)# labels for x axis
# labels above bars:
labels_aboveBars <- xgboost_explainInstance2$contribution %>% 
  round(2) %>% 
  as.character() 
# labels_aboveBars[1] <- "-" # "delete intercept value
# labels_aboveBars[length(labels_aboveBars)] <- "-" # "delete" prediction value

ggplot(data = xgboost_explainInstance2, mapping = aes(x = position, y = cumulative, fill = sign)) +
    geom_col()+
    coord_flip()+
    scale_x_discrete(labels = labels) +
    scale_fill_manual(values = c(
      "intercept" = "#e6cea0",
      "negative" = "#db4646",
      "positive" = "#24b3d0",
      "Prediction" = "#684b8e"
    ))+
    geom_text(aes(label = labels_aboveBars), 
          size = 4, 
          color = "black", 
          fontface = "bold")+
    xlab("Variable") +
    ylab("GFR")+
    theme_minimal() +
    theme(
  axis.title.x = element_text(size = 16),
  axis.title.y = element_text(size = 16)
)+
  theme(
  plot.title = element_text(color = "black", size = 18) 
) +
  theme(legend.position = "none") +
  ggtitle("B) Patient with normal GFR")
```

::: {.fig_legend data-latex=""}
**Figura 4**. Descomposici√≥n del valor de tasa de filtraci√≥n glomerular (GFR) estimado por el modelo XGBoost. A) Paciente con tasa de filtraci√≥n glomerular baja (low GFR). B) Paciente con tasa de filtraci√≥n glomerular normal (normal GFR). Las barras azules indican una contribuci√≥n positiva al valor de la predicci√≥n, las rojas indican una contribuci√≥n negativa. El valor de la predicci√≥n para la tasa de filtraci√≥n glomerular corresponde a la barra violeta. La contribuci√≥n de cada variable al valor de la predicci√≥n se muestra a la derecha de la cada barra. El valor de partida es el t√©rmino independiente de la regresi√≥n (intercept, barra amarilla). Creatine, creatinina; BUN, Nitr√≥geno ur√©ico en sangre; Protein_in_Urine, prote√≠na en orina; CKD_Status_Yes, presencia de enfermedad renal cr√≥nica; Age, edad; Diabetes_Yes, presencia de diabetes; Hypertension_Yes, presencia de hipertensi√≥n; Urine_Output, salida de orina; Water_Intake, ingesta de agua; Medication, tipo de medicaci√≥n (ARB; Diuretic, diur√©ticos; None, ninguna).
:::

```{r}
#| message: false
#| warning: false
#| include: FALSE

# the output is a DF with variables and their contributions
randomF_breakdown <- predict_parts(explainer = randomForest_explainer, 
                    new_observation = instance1, type = "break_down")

randomF_breakdown2 <- predict_parts(explainer = randomForest_explainer, 
                    new_observation = instance2, type = "break_down")
```

```{r}
#| echo: FALSE
#| message: false
#| warning: false
randomF_explainInstance <- randomF_breakdown %>%
  as_tibble()
# Add label to variable name:
randomF_explainInstance[nrow(randomF_explainInstance), 3] <- "Prediction"
randomF_explainInstance[1, 6] <- "0" # change "sing" of the intercept
randomF_explainInstance <- randomF_explainInstance %>%
  select(3, 2, 5, 6, 7) %>% 
  # reverse the order of position for better visualization after coord flip:
  mutate(position = factor(position, levels = rev(position))) %>% 
    mutate(sign = case_when( # make more intuitve factors
    sign == 0 ~"intercept",
    sign == -1 ~"negative",
    sign == 1 ~"positive",
    sign == "X" ~"Prediction"
  ))
  

labels <- rev(randomF_explainInstance$variable_name)# labels for x axis
# labels above bars:
labels_aboveBars <- randomF_explainInstance$contribution %>% 
  round(2) %>% 
  as.character() 
# labels_aboveBars[1] <- "-" # "delete intercept value
# labels_aboveBars[length(labels_aboveBars)] <- "-" # "delete" prediction value

ggplot(data = randomF_explainInstance, mapping = aes(x = position, y = cumulative, fill = sign)) +
    geom_col()+
    coord_flip()+
    scale_x_discrete(labels = labels) +
    scale_fill_manual(values = c(
      "intercept" = "#e6cea0",
      "negative" = "#db4646",
      "positive" = "#24b3d0",
      "Prediction" = "#684b8e"
    ))+
    geom_text(aes(label = labels_aboveBars), 
          size = 4, 
          color = "black", 
          fontface = "bold")+
    xlab("Variable") +
    ylab("GFR")+
    theme_minimal() +
    theme(
  axis.title.x = element_text(size = 16),
  axis.title.y = element_text(size = 16)
)+
  theme(
  plot.title = element_text(color = "black", size = 18) 
) +
  theme(legend.position = "none") +
  ggtitle("A) Patient with low GFR")
```

```{r}
#| echo: FALSE
#| message: false
#| warning: false
randomF_explainInstance2 <- randomF_breakdown2 %>%
  as_tibble() %>% 
  mutate(sign = as.character(sign))
# Add label to variable name:
randomF_explainInstance2[nrow(randomF_explainInstance2), 3] <- "Prediction"
randomF_explainInstance2[1, 6] <- "2" # change "sing" of the intercept
randomF_explainInstance2 <- randomF_explainInstance2 %>%
  select(3, 2, 5, 6, 7) %>% 
  # reverse the order of position for better visualization after coord flip:
  mutate(position = factor(position, levels = rev(position))) %>% 
    mutate(sign = case_when( # make more intuitve factors
    sign == 2 ~"intercept",
    sign == -1 ~"negative",
    sign == 0 ~"positive",
    sign == 1 ~"positive",
    sign == "X" ~"Prediction"
  ))
  

labels <- rev(randomF_explainInstance2$variable_name)# labels for x axis
# labels above bars:
labels_aboveBars <- randomF_explainInstance2$contribution %>% 
  round(2) %>% 
  as.character() 
# labels_aboveBars[1] <- "-" # "delete intercept value
# labels_aboveBars[length(labels_aboveBars)] <- "-" # "delete" prediction value

ggplot(data = randomF_explainInstance2, mapping = aes(x = position, y = cumulative, fill = sign)) +
    geom_col()+
    coord_flip()+
    scale_x_discrete(labels = labels) +
    scale_fill_manual(values = c(
      "intercept" = "#e6cea0",
      "negative" = "#db4646",
      "positive" = "#24b3d0",
      "Prediction" = "#684b8e"
    ))+
    geom_text(aes(label = labels_aboveBars), 
          size = 4, 
          color = "black", 
          fontface = "bold")+
    xlab("Variable") +
    ylab("GFR")+
    theme_minimal() +
    theme(
  axis.title.x = element_text(size = 16),
  axis.title.y = element_text(size = 16)
)+
  theme(
  plot.title = element_text(color = "black", size = 18) 
) +
  theme(legend.position = "none") +
  ggtitle("B) Patient with normal GFR")
```

::: {.fig_legend data-latex=""}
**Figura 5**. Descomposici√≥n del valor de tasa de filtraci√≥n glomerular (GFR) estimado por el modelo de bosque de √°rboles aleatorios. A) Paciente con tasa de filtraci√≥n glomerular baja (low GFR). B) Paciente con tasa de filtraci√≥n glomerular normal (normal GFR). Las barras azules indican una contribuci√≥n positiva al valor de la predicci√≥n, las rojas indican una contribuci√≥n negativa. El valor de la predicci√≥n para la tasa de filtraci√≥n glomerular corresponde a la barra violeta. La contribuci√≥n de cada variable al valor de la predicci√≥n se muestra a la derecha de la cada barra. El valor de partida es el t√©rmino independiente de la regresi√≥n (intercept, barra amarilla). Creatine, creatinina; BUN, Nitr√≥geno ur√©ico en sangre; Protein_in_Urine, prote√≠na en orina; CKD_Status_Yes, presencia de enfermedad renal cr√≥nica; Age, edad; Diabetes_Yes, presencia de diabetes; Hypertension_Yes, presencia de hipertensi√≥n; Urine_Output, salida de orina; Water_Intake, ingesta de agua; Medication, tipo de medicaci√≥n (ARB; Diuretic, diur√©ticos; None, ninguna).
:::

Las Figuras 4 y 5 muestran que los valores de la tasa de filtraci√≥n glomerular estimados por ambos modelos para los dos pacientes (4.799 con XGBoost y 5.41 con el bosque de √°rboles aleatorios para el paciente con tasa de filtraci√≥n glomerular baja; 103.808 con XGBoost y 102.93 con el bosque de √°rboles aleatorios para el paciente con tasa de filtraci√≥n glomerular normal) se acercan mucho a los valores reales (5.00 para el paciente elegido con una tasa de filtraci√≥n baja y 103.00 para el paciente con una tasa de filtraci√≥n normal).

En ambas figuras vemos adem√°s como casi todas las variables que son importantes para los dos modelos (Figura 3) contribuyen a reducir (Figuras 4A y 5A) o aumentar (Figuras 4B y 5B) progresivamente el valor de la predicci√≥n, dependiendo del paciente.

::: {.infobox data-latex=""}
> üî¨ En nuestro contexto, los gr√°ficos de descomposici√≥n del valor de la regresi√≥n podr√≠an ayudarnos a establecer un plan de cuidados personalizado para el paciente, orient√°ndonos a disminuir sus niveles de creatinina, nitr√≥geno ur√©ico en sangre y prote√≠nas en la orina, pues estas variables son las que reducen en mayor medida el valor de la predicci√≥n para la tasa de filtraci√≥n glomerular.
:::

## üìù Usamos las piezas del puzzle para hacer recomendaciones {style="font-size: 18px"}

Los gr√°ficos de importancia de variables (Figura 3) y los gr√°ficos de descomposici√≥n del valor de la regresi√≥n (Figuras 4 y 5) nos ayudan a comprender qu√© variables son las m√°s importantes para las predecir una variable de respuesta, tanto a nivel global (gr√°ficos de importancia de variables) como a nivel de casos individuales (gr√°ficos de descomposici√≥n de la regresi√≥n). A trav√©s del Aprendizaje Autom√°tico, podemos usar esta informaci√≥n para modelar c√≥mo cambiar√≠a el valor de una predicci√≥n al modificar los valores de variables predictoras espec√≠ficas.

El an√°lisis *Ceteris Paribus* examina c√≥mo cambios en una variable afectar√≠an la predicci√≥n de un modelo, *manteniendo todas las dem√°s variables constantes* (i.e., *Ceteris Paribus*). La funci√≥n `predict_profile()` de DALEX nos devuelve los datos necesarios para generar gr√°ficos *Ceteris Paribus* para casos individuales.

La Figura 6 muestra gr√°ficos *Ceteris Paribus* para dos pacientes elegidos al azar dentro del grupo de pacientes con baja tasa de filtraci√≥n glomerular. Modelamos los valores de la predicci√≥n de esta variable en funci√≥n de las variables que ya hemos identificado como las m√°s importnates: creatinina, nitr√≥geno ur√©ico en sangre y prote√≠na en la orina. Para mejorar las comparaciones, en el an√°lisis *Ceteris Paribus,* los valores de las variables se muestran como puntuaciones *z* (valores estandarizados). Para regresar a las unidades de la escala original, podemos multiplicar los valores por la desviaci√≥n est√°ndar y sumar la media del conjunto de datos.

```{r}
#| echo: FALSE
#| message: false
#| warning: false

cp_instance1 <- predict_profile(xgboost_explainer,
                                new_observation = instance1)



plot(cp_instance1, variables = c("Creatinine", "BUN", "Protein_in_Urine")) +
  theme_minimal() +
  labs(x = "z-score", y = "Predicted GFR") +
    theme(
      axis.title.x = element_text(size = 16),
      axis.title.y = element_text(size = 16)
    ) +
  theme(
    plot.title = element_text(color = "black", size = 18)
  ) +
  theme(plot.subtitle = element_blank()) +
  ggtitle("Patient 1")



 

# New Patient
set.seed(1976)
instance_low2 <- DF_lowGFR[sample(nrow(DF_lowGFR), 1), ] # random row
instance_high2 <- DF_highGFR[sample(nrow(DF_highGFR), 1), ] # random row
# combine two cases
subjects2 <- bind_rows(instance_low2, instance_high2)
# bake these new data for analysis
explainer_subjects2 <- R_CorrOk %>%
prep(training = training_data) %>%
bake(new_data = subjects2) %>%
select(GFR, everything())


instance1.2 <- explainer_subjects2[1,]
instance2.2 <- explainer_subjects2[2,]


cp_instance1.2 <- predict_profile(xgboost_explainer,
                                new_observation = instance1.2)

plot(cp_instance1.2, variables = c("Creatinine", "BUN", "Protein_in_Urine")) +
  theme_minimal() +
  labs(x = "z-score", y = "Predicted GFR") +
  theme(
    axis.title.x = element_text(size = 16),
    axis.title.y = element_text(size = 16)
  ) +
  theme(
    plot.title = element_text(color = "black", size = 18)
  ) +
  theme(plot.subtitle = element_blank()) +
  ggtitle("Patient 2")

```

::: {.fig_legend data-latex=""}
**Figura 6**. Gr√°ficos *Ceteris Paribus* para dos pacientes con baja tasa de filtraci√≥n glomerular. El eje X corresponde a las puntuaciones *z* (z-score, valores escalados) de cada una de las variables. El eje Y corresponde al valor de la predicci√≥n (prediction) de la tasa de filtraci√≥n glomerular asociada a cada puntuaci√≥n *z.* Los puntos violeta corresponden al valor real de *z* asociado a cada variable. Paciente 1 (Patient 1): BUN, *z* = 2.49 = 108 mg/dL; Creatinine, *z* = 1.89 = 5.83 mg/dL; Pretein_in_Urine, *z* = 0.621 = 1058 mg/dL. Paciente 2 (Patient 2): BUN, *z* = 0.852 = 57 mg/dL; Creatinine, *z* = 2.52 = 7.11 mg/dL; Pretein_in_Urine, *z* = 2.35 = 2493 mg/dL. Creatine, creatinina; BUN, Nitr√≥geno ur√©ico en sangre; Protein_in_Urine, prote√≠na en orina.
:::

La Figura 6 muestra que, manteniendo el resto de las variables sin cambios y teniendo en cuenta los valores reales del resto de las variables asociadas a CADA paciente, para mejorar la tasa de flitraci√≥n glomerular, disminuir los valores del nitr√≥geno ur√©ico en sangre (BUN) tendr√≠a un efecto mayor en el paciente 1 que en el paciente 2.

La Figura 6 tambi√©n revela que la mejor estrategia para mejorar la tasa de filtraci√≥n glomerular ser√≠a disminuir los niveles de creatinina en los dos pacientes. Aunque los niveles de prote√≠na en la orina parecen tener un efecto marginal, parece haber un umbral, al rededor de *z* = 0.3 = 782 mg/dL, por debajo del cual el valor estimado de la tasa de filtraci√≥n glomerular mejora considerablemente.

```         
```

::: {.infobox data-latex=""}
> üöÄ Combinando los gr√°ficos de descomposici√≥n de la regresi√≥n con el an√°lisis *Ceteris Paribus* podemos ofrecer recomendaciones muy precisas para establecer estrategias informadas que nos permitan modificar la variable de respuesta de la forma m√°s inteligente posible.
:::

```         
```

# Conclusiones

-   A pesar de que el funcionamiento interno de algoritmos como XGBoost y el bosque de √°rboles aleatorios es complejo y nada transparente sobre la forma en la que alcanzan una predicci√≥n, la librer√≠a DALEX nos permite identificar f√°cilmente:

    -   Las variables que son m√°s importantes para hacer predicciones sobre la variable de respuesta.

    -   A nivel de casos individuales, podemos cuantificar la contribuci√≥n de cada variable predictora al valor falor de la predicci√≥n.

    -   Tambi√©n a nivel de casos individuales, podemos modelar c√≥mo cambiar√≠a el valor de la predicci√≥n al modificar los valores de variables predictoras espec√≠ficas.

-   Combinando esta informaci√≥n, sin importar lo complejo que sea el modelo de Aprendizaje Autom√°tico, podemos ofrecer a nivel de casos individuales, recomendaciones precisas y cuantificables para modificar la variable de respuesta a trav√©s de estrategias informadas.

El c√≥digo completo de este an√°lisis est√° disponible en el repositorio de [La Casa de Datos](https://github.com/lacasadedatos/R_scripts "Ver c√≥digo del an√°lisis") en Github.
